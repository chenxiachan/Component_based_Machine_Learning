{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the open source code of paper: Utilizing Domain Knowledge: Robust Machine Learning for Building Energy Prediction with Small, Inconsistent Datasets.\n",
    "- @Date : 2022-06-24 16:21:35\n",
    "- @Link : https://arxiv.org/abs/2302.10784\n",
    "- @Ver : v02\n",
    "- @Author: Xia CHEN (xia.chen@iek.uni-hannover.de), Xia Chen, Manav Mahan Singh, Philipp Geyer\n",
    "For using the code or data, please cite:\n",
    "\n",
    "*- Chen, X., Singh, M.M. and Geyer, P., 2023. Utilizing Domain Knowledge: Robust Machine Learning for Building Energy Prediction with Small, Inconsistent Datasets. arXiv preprint arXiv:2302.10784.*\n",
    "\n",
    "*- CHEN, Xia; Singh, Manav Mahan; Geyer, Philipp (2023), “Utilizing domain knowledge: robust machine learning for building energy performance prediction with small, inconsistent datasets”, Mendeley Data, V2, doi: 10.17632/fctghwx3r9.2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This code is designed for implementing the core of Component-based Machine Learning (CBML) framework, by using previous component and zone level ML model for predicting building level heat flows and their impact on the building's heating load. Available ML models are: `DecisionTree`, `SGDRegressor`, `LightGBM`. They are involved in proving that the effectiveness of CBML framework is model-independent. The process is divided into several distinct stages:\n",
    "\n",
    "---\n",
    "1. **Data Preparation**: The code starts by preparing the data necessary for the prediction process. It reads a CSV file containing the building zones' typical day characteristics for a given period (`df_route`). The dataframe is initially augmented with an identifier column (`'id'`) that combines file names and zone names to uniquely identify each entry.\n",
    "---\n",
    "2. **Feature Selection and Model Prediction**: The script defines a set of input features relevant to the building's thermal characteristics and external conditions, including month, day, hour, temperature, humidity, and various building design parameters like wall area and window-to-wall ratio (WWR). These features are used to predict the heat flows through different components (roof, wall/window, infiltration, ground floor) using pretrained LightGBM models.\n",
    "---\n",
    "3. **Component Heat Flow Calculation**: For each building component (roof, wall/window, infiltration, ground floor), a separate CSV file with predictions is read. The predictions are processed to calculate the total heat flow from each component, taking into account the number of occurrences of each file in the dataset to properly replicate the data and ensure consistency in the aggregation process.\n",
    "---\n",
    "4. **Aggregation and Final Prediction**: After calculating the component heat flows, these values are merged with the original dataset. This enriched dataset now serves as input to another ML model to predict the overall heating load of the building. This stage signifies the integration of component-level predictions into a holistic assessment of the building's thermal performance.\n",
    "---\n",
    "5. **Performance Evaluation**: Finally, the predicted heating load is compared against actual values to evaluate the model's performance using various metrics (RMSE, SMAPE, MAE, MSE, R^2). This evaluation provides insights into the accuracy and reliability of the model predictions.\n",
    "---\n",
    "6. **Date Handling and Result Output**: The script also handles date information by combining year, month, day, and hour into a single datetime object. This datetime information, alongside the predicted and actual heating load values, can be used for further analysis or visualizations.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['BuildingTypicalDays_Winter Typical.csv',\n",
       " 'GFloorTypicalDays_Winter Typical.csv',\n",
       " 'InfiltrationTypicalDays_Winter Typical.csv',\n",
       " 'RoofTypicalDays_Winter Typical.csv',\n",
       " 'WallWindowTypicalDays_Winter Typical.csv',\n",
       " 'ZoneTypicalDays_Winter Typical.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building performance simulation, component\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import csv\n",
    "from math import ceil\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.offline as py                    \n",
    "py.init_notebook_mode(connected=True)          \n",
    "import plotly.graph_objs as go                 \n",
    "import plotly.figure_factory as ff             \n",
    "import plotly.io as pio                        \n",
    "\n",
    "# Target plot\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "# model training\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# pd.set_option('max_columns', 50)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "ROUTE = '../_TrainData_done/'\n",
    "ROUTE_test = '../_TestData_done/'\n",
    "\n",
    "g = os.walk(ROUTE)  \n",
    "\n",
    "files = []\n",
    "for path,dir_list,file_list in g:  \n",
    "    for file_name in file_list:  \n",
    "        files.append(file_name)\n",
    "\n",
    "\n",
    "PERIOD = files[0].split('_')[1][:-4]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers #################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# 创建文件夹\n",
    "def mkdir(path):\n",
    "    path=path.strip()\n",
    "    path=path.rstrip(\"\\\\\")\n",
    "    isExists=os.path.exists(path)\n",
    "    if not isExists:\n",
    "        os.makedirs(path) \n",
    "        \n",
    "# Result & Evaluation\n",
    "def RMSE(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "\n",
    "def SMAPE(F, A):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "\n",
    "def MAE(F, A):\n",
    "    return 1/len(A) * np.sum(np.abs(F - A))\n",
    "\n",
    "\n",
    "def MSE(F, A):\n",
    "    return 1/len(A) * np.sum(pow(np.abs(F - A),2))\n",
    "\n",
    "# check the value distribution of training output, it's important for hyperparameter tuning\n",
    "def plot_target(df, TARGET):\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plot_df = df[TARGET]\n",
    "    # his_grid = random.sample(list(plot_df), 10000)\n",
    "\n",
    "    sns.distplot(plot_df,label=TARGET)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params #################################################################################\n",
    "\n",
    "def train_test_df(component, df, df_test, lgb_params, features, TARGET):  \n",
    "    # spilt Train/Test\n",
    "    X_train = df[features]\n",
    "    y_train = df[TARGET]\n",
    "\n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test[TARGET]\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "#     # Test on different ML models\n",
    "#     scaler = StandardScaler()  \n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "#     # save the scaler\n",
    "#     regr = tree.DecisionTreeRegressor()\n",
    "#     regr.fit(X_train, y_train)\n",
    "    \n",
    "#     y_pred = regr.predict(X_test)\n",
    "#     print('DecisionTree',r2_score(y_test, y_pred))\n",
    "\n",
    "#     regr= linear_model.SGDRegressor()\n",
    "#     regr.fit(X_train, y_train)\n",
    "#     y_pred = regr.predict(X_test)\n",
    "#     print('SGDRegressor', r2_score(y_test, y_pred))\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, \n",
    "                       label=y_train)\n",
    "    # train_data.save_binary('train_data.bin')\n",
    "    # train_data = lgb.Dataset('train_data.bin')\n",
    "\n",
    "    # valid_data = lgb.Dataset(vaild_df[features], \n",
    "    #                    label=vaild_df[Target_features[0]])\n",
    "\n",
    "    estimator = lgb.cv(\n",
    "                        lgb_params,\n",
    "                        train_data,\n",
    "                        num_boost_round=10000,\n",
    "                        nfold=3,\n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval=100,\n",
    "                        stratified=False,\n",
    "                        seed=42\n",
    "    )\n",
    "    rounds = int(len(estimator['rmse-mean']))\n",
    "    print('The best RMSE in CV is {:.5f}，std {:.5f}.'.format(\n",
    "    estimator['rmse-mean'][-1], estimator['rmse-stdv'][-1]))\n",
    "\n",
    "    print('The best iteration round is {}.'.format(len(estimator['rmse-mean'])))\n",
    "    \n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          verbose_eval = 100,\n",
    "                          num_boost_round = int(len(estimator['rmse-mean'])), ###########\n",
    "                          )\n",
    "    ############################ Making result df ############################\n",
    "    test_df = pd.DataFrame(y_test)\n",
    "    test_df['pred'] = estimator.predict(X_test)\n",
    "\n",
    "    ############################ Result evaluation ###########################\n",
    "    msg = 'Result RMSE is {}, SMAPE is {}, MAE is {}, MSE is {}, R^2 is {}'.format('%.4f' % RMSE(test_df['pred'],test_df[TARGET]), \n",
    "                                                             '%.4f' % SMAPE(test_df['pred'],test_df[TARGET]), \n",
    "                                                             '%.4f' % MAE(test_df['pred'],test_df[TARGET]),\n",
    "                                                             '%.4f' % MSE(test_df['pred'],test_df[TARGET]),\n",
    "                                                             '%.4f' % metrics.r2_score(test_df[TARGET],test_df['pred']))\n",
    "    print(msg)\n",
    "    ############################ Make dir ###########################\n",
    "    mkdir(component + '_' + TARGET)\n",
    "    \n",
    "    ########################### Feature importance ##########################\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "    # mean decrease impurity\n",
    "    imp_img = lgb.plot_importance(estimator, ignore_zero=False)\n",
    "    imp_img.figure.savefig(component + '_' + TARGET + '/' + TARGET + '_feature_imp.png')\n",
    "#     imp_img.save_img(TARGET + '/' + TARGET + '_feature_imp.png')\n",
    "    ########################### save model ##############################\n",
    "    estimator.save_model(component + '_' + TARGET + '/' + TARGET + '_model.txt')\n",
    "#     #load from model:\n",
    "#     bst = lgb.Booster(model_file='mode.txt')\n",
    "    ########################### log result ##############################\n",
    "    with open(\"result.csv\",\"a+\", newline='') as csvfile: \n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows([[component, TARGET, str(rounds), str(RMSE(test_df['pred'],test_df[TARGET])), str(SMAPE(test_df['pred'],test_df[TARGET])),str(MAE(test_df['pred'],test_df[TARGET])), str(MSE(test_df['pred'],test_df[TARGET])), str(metrics.r2_score(test_df[TARGET],test_df['pred'])), len(features)]])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### component prediction ###############################################\n",
    "def load_component_predict(model_route, features, df_route, num_dict, TARGET, REPEAT=False):\n",
    "#     bst_roof = lgb.Booster(model_file='model/Heat Flow_GFloor.txt')\n",
    "#     bst_infi = lgb.Booster(model_file='model/Heat Flow_Infiltration.txt')\n",
    "#     bst_wallwin = lgb.Booster(model_file='model/Heat Flow_WallWindow.txt')\n",
    "#     bst = lgb.Booster(model_file='model/Heat Flow_Roof.txt')\n",
    "#     X = df_test['Area', 'U Value', 'Heat Capacity']\n",
    "#     df_test['pred'] = bst.predict(X)\n",
    "    df = pd.read_csv(df_route)\n",
    "#     df = df.set_index('File',drop=True)\n",
    "    df.head()\n",
    "    \n",
    "    bst = lgb.Booster(model_file=model_route)\n",
    "    X = df[features]\n",
    "    df['pred'] = bst.predict(X)\n",
    "    df['id'] = df['File'] + df['Zone Name']\n",
    "    \n",
    "    if(REPEAT is True):\n",
    "        new_df = pd.DataFrame()\n",
    "        for a,b in zip(num_dict.index.tolist(), num_dict['File']):\n",
    "            b = int(b/312)\n",
    "            for i in range(0,b):\n",
    "                new_df = pd.concat([new_df,df[df['File'] == a]],axis=0)\n",
    "    else:\n",
    "        new_df = df\n",
    "\n",
    "    new_df = new_df[['id', TARGET + ':Heat Flow','pred','File']]\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../_TestData_done/ZoneTypicalDays_Winter Typical.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shape101_17</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_0</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_16</th>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_5</th>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_18</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_192</th>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_195</th>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_183</th>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_191</th>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shape101_189</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              File\n",
       "Shape101_17   1560\n",
       "Shape101_0    1560\n",
       "Shape101_16    936\n",
       "Shape101_5    1248\n",
       "Shape101_18   1560\n",
       "...            ...\n",
       "Shape101_192   936\n",
       "Shape101_195  1248\n",
       "Shape101_183  1248\n",
       "Shape101_191   936\n",
       "Shape101_189  1560\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_route = ROUTE_test + 'ZoneTypicalDays_' + PERIOD + '.csv'\n",
    "print(df_route)\n",
    "df = pd.read_csv(df_route)\n",
    "df['id'] = df['File'] + df['Name']\n",
    "df.head()\n",
    "\n",
    "Input_features = ['Month', 'Day', 'Hour', 'Temp', 'Dew', 'Hum', 'Pres', \n",
    "                 'id', 'Internal Wall Area', 'Internal Floor Area', 'Heat Capacity (Floor Slabs)', \n",
    "                 'Internal Mass', 'WWR (North)', 'WWR (East)', 'WWR (West)', 'WWR (South)', \n",
    "                 'Building_Start Time', 'Building_Operating Hours', 'Building_Light Heat Gain', \n",
    "                 'Building_Equipment Heat Gain', 'Building_Occupancy', 'Building_Heating Setpoint', 'Building_Cooling Setpoint','Wall Heat Flow', 'Window Heat Flow']\n",
    "\n",
    "num_dict = pd.DataFrame(df['File'].value_counts())\n",
    "num_dict = num_dict.loc[df['File'].drop_duplicates().tolist()]\n",
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248000, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Winter Typical'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERIOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew</th>\n",
       "      <th>Hum</th>\n",
       "      <th>Pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-01-05 01:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>82</td>\n",
       "      <td>95700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05 02:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-8.4</td>\n",
       "      <td>88</td>\n",
       "      <td>95700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05 03:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>-7.9</td>\n",
       "      <td>89</td>\n",
       "      <td>95700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05 04:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>-10.7</td>\n",
       "      <td>80</td>\n",
       "      <td>95600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-05 05:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.9</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>82</td>\n",
       "      <td>95600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-17 20:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>95</td>\n",
       "      <td>96300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-17 21:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>92</td>\n",
       "      <td>96400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-17 22:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>93</td>\n",
       "      <td>96500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-17 23:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>95</td>\n",
       "      <td>96600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-01-18 00:00:00</th>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>95</td>\n",
       "      <td>96600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Year  Month  Day  Hour  Temp   Dew  Hum   Pres\n",
       "1986-01-05 01:00:00  1986      1    5     1  -5.4  -7.7   82  95700\n",
       "1986-01-05 02:00:00  1986      1    5     2  -6.9  -8.4   88  95700\n",
       "1986-01-05 03:00:00  1986      1    5     3  -6.6  -7.9   89  95700\n",
       "1986-01-05 04:00:00  1986      1    5     4  -8.2 -10.7   80  95600\n",
       "1986-01-05 05:00:00  1986      1    5     5  -7.9 -10.2   82  95600\n",
       "...                   ...    ...  ...   ...   ...   ...  ...    ...\n",
       "1986-01-17 20:00:00  1986      1   17    20  -2.1  -2.7   95  96300\n",
       "1986-01-17 21:00:00  1986      1   17    21  -1.9  -2.9   92  96400\n",
       "1986-01-17 22:00:00  1986      1   17    22  -4.2  -5.0   93  96500\n",
       "1986-01-17 23:00:00  1986      1   17    23  -4.9  -5.5   95  96600\n",
       "1986-01-18 00:00:00  1986      1   17    24  -4.4  -5.0   95  96600\n",
       "\n",
       "[312 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge with weather data\n",
    "\n",
    "weather_df = pd.read_csv('../Weather_data.csv')\n",
    "# to date_time and set as index\n",
    "date_df = weather_df[['Year', 'Month', 'Day', 'Hour']]\n",
    "date = pd.to_datetime(date_df)\n",
    "\n",
    "weather_df.index = date\n",
    "\n",
    "del date_df, date\n",
    "\n",
    "weather_df = weather_df[['Year', 'Month', 'Day', 'Hour','Temp','Dew','Hum','Pres']]\n",
    "if(PERIOD == 'Winter Typical'):\n",
    "    # 1/5-1/17\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 1)&(weather_df['Day'] >= 5)&(weather_df['Day'] <= 17)]\n",
    "if(PERIOD == 'Winter Extreme'):\n",
    "    # 2/9-2/21\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 2)&(weather_df['Day'] >= 9)&(weather_df['Day'] <= 21)]\n",
    "if(PERIOD == 'Spring'):\n",
    "    # 3/28-4/10\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 2)&(weather_df['Day'] >= 9)&(weather_df['Day'] <= 21)]\n",
    "if(PERIOD == 'Summer Typical'):\n",
    "    # 7/12 - 7/24\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 7)&(weather_df['Day'] >= 12)&(weather_df['Day'] <= 24)]\n",
    "if(PERIOD == 'Summer Extreme'):\n",
    "    # 7/19-7/31\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 7)&(weather_df['Day'] >= 19)&(weather_df['Day'] <= 31)]\n",
    "if(PERIOD == 'Autumn'):\n",
    "    # 10/19-10/31\n",
    "    target_weather = weather_df.loc[(weather_df['Month'] == 10)&(weather_df['Day'] >= 19)&(weather_df['Day'] <= 31)]\n",
    "target_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6240000, 4)\n",
      "Roof Done!\n",
      "(10608000, 4)\n",
      "(1248000, 3)\n",
      "WallWindow Done!\n",
      "(1248000, 4)\n",
      "Infiltration Done!\n",
      "(1248000, 4)\n",
      "GFloor Done!\n"
     ]
    }
   ],
   "source": [
    "# component prediction\n",
    "\n",
    "# features = ['Month','Day','Hour','Temp','Dew','Hum',\n",
    "#             'Pres', 'Area', 'U Value', 'Heat Capacity']\n",
    "\n",
    "TARGET =  PERIOD + '_Heat Flow'\n",
    "result_route = files[3][:-4] + '_' + TARGET + '/' + TARGET + '.csv'\n",
    "df_roof = pd.read_csv(result_route)\n",
    "\n",
    "df_roof = df_roof[['File', 'Zone Name', 'Name', 'pred']]\n",
    "df_roof.columns = ['File', 'Zone Name', 'Name', 'Roof Heat Flow']\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for a,b in zip(num_dict.index.tolist(), num_dict['File']):\n",
    "    b = int(b/312)\n",
    "    for i in range(0,b):\n",
    "        new_df = pd.concat([new_df,df_roof[df_roof['File'] == a]],axis=0)\n",
    "df_roof = new_df\n",
    "\n",
    "\n",
    "print(df_roof.shape)\n",
    "print('Roof Done!')\n",
    "\n",
    "######################################\n",
    "\n",
    "result_route = files[4][:-4] + '_' + TARGET + '/' + TARGET + '.csv'\n",
    "df_ww = pd.read_csv(result_route)\n",
    "\n",
    "df_ww = df_ww[['File', 'Zone Name', 'Name', 'pred']]\n",
    "df_ww.columns = ['File', 'Zone Name', 'Name', 'WallWindow Heat Flow']\n",
    "print(df_ww.shape)\n",
    "# merge target_weather\n",
    "target_weather = target_weather.reset_index(drop=True)\n",
    "repeat_time = int(len(df_ww)/len(target_weather))\n",
    "\n",
    "merge_weather = pd.concat([target_weather]*repeat_time, ignore_index=True)\n",
    "# output_df = output_df.reset_index(drop=True)\n",
    "df_ww = pd.concat([merge_weather, df_ww], axis=1)\n",
    "df_ww['id'] = df_ww['File'] + df_ww['Zone Name'] + df_ww['Month'].values.astype(str).tolist() + df_ww['Day'].values.astype(str).tolist() + df_ww['Hour'].values.astype(str).tolist()\n",
    "df_ww['WallWindow Heat Flow'] = df_ww.groupby(by='id')['WallWindow Heat Flow'].transform('sum')\n",
    "df_ww = df_ww[['WallWindow Heat Flow','id']]\n",
    "df_ww = df_ww.drop_duplicates(subset=['id'])\n",
    "\n",
    "df_ww = df_ww.reset_index()\n",
    "print(df_ww.shape)\n",
    "print('WallWindow Done!')\n",
    "\n",
    "\n",
    "result_route = files[2][:-4] + '_' + TARGET + '/' + TARGET + '.csv'\n",
    "df_inf = pd.read_csv(result_route)\n",
    "\n",
    "df_inf = df_inf[['File', 'Zone Name', 'Name', 'pred']]\n",
    "df_inf.columns = ['File', 'Zone Name', 'Name', 'Infiltration Heat Flow']\n",
    "print(df_inf.shape)\n",
    "print('Infiltration Done!')\n",
    "\n",
    "\n",
    "result_route = files[1][:-4] + '_' + TARGET + '/' + TARGET + '.csv'\n",
    "df_gfloor = pd.read_csv(result_route)\n",
    "\n",
    "df_gfloor = df_gfloor[['File', 'Zone Name', 'Name', 'pred']]\n",
    "df_gfloor.columns = ['File', 'Zone Name', 'Name', 'GFloor Heat Flow']\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for a,b in zip(num_dict.index.tolist(), num_dict['File']):\n",
    "    b = int(b/312)\n",
    "    for i in range(0,b):\n",
    "        new_df = pd.concat([new_df,df_gfloor[df_gfloor['File'] == a]],axis=0)\n",
    "df_gfloor = new_df\n",
    "\n",
    "\n",
    "print(df_gfloor.shape)\n",
    "print('GFloor Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZoneTypicalDays_Winter Typical_Winter Typical_Heat Flow/Winter Typical_Heat Flow_model.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_route = files[5][:-4] + '_' + TARGET + '/' + TARGET + '_model.txt'\n",
    "result_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# merge component heat flow ############\n",
    "X = df[Input_features]\n",
    "X = pd.concat([X,df_roof['Roof Heat Flow'].reset_index()], axis=1)\n",
    "X = pd.concat([X,df_ww['WallWindow Heat Flow'].reset_index()], axis=1)\n",
    "X = pd.concat([X,df_inf['Infiltration Heat Flow'].reset_index()], axis=1)\n",
    "X = pd.concat([X,df_gfloor['GFloor Heat Flow'].reset_index()], axis=1)\n",
    "X = X.fillna(0)\n",
    "\n",
    "########################### load zone area & predict ##############\n",
    "features = ['Month', 'Day', 'Hour', 'Temp', 'Dew', 'Hum', 'Pres', \n",
    "             'Internal Wall Area', 'Internal Floor Area', 'Heat Capacity (Floor Slabs)', \n",
    "             'Internal Mass', 'WWR (North)', 'WWR (East)', 'WWR (West)', 'WWR (South)', \n",
    "             'Building_Start Time', 'Building_Operating Hours', 'Building_Light Heat Gain', \n",
    "             'Building_Equipment Heat Gain', 'Building_Occupancy', 'Building_Heating Setpoint', 'Building_Cooling Setpoint',\n",
    "             'Wall Heat Flow', 'Window Heat Flow', 'WallWindow Heat Flow', 'GFloor Heat Flow', 'Roof Heat Flow', 'Infiltration Heat Flow']\n",
    "\n",
    "\n",
    "X = X[features]\n",
    "X = X.round(3)\n",
    "# route_list = ['Heating Load','Cooling Load','Lights Load']\n",
    "route_list = [PERIOD+':Heating Load']\n",
    "# load model\n",
    "for each_route in route_list:\n",
    "#     model_route = 'model/Heating Load_Zone.txt'\n",
    "#     model_route = 'model/'+ each_route + '_Zone.txt'\n",
    "    TARGET = PERIOD + '_Heating Load'\n",
    "    result_route = files[5][:-4] + '_' + TARGET + '/' + TARGET + '_model.txt'\n",
    "    bst = lgb.Booster(model_file=result_route)\n",
    "\n",
    "    # predict\n",
    "    df['pred'] = bst.predict(X)\n",
    "    # ['WallWindow Heat Flow', 'GFloor Heat Flow', 'Roof Heat Flow', 'Infiltration Heat Flow']\n",
    "\n",
    "    test_df = df[[each_route, 'pred']]\n",
    "    TARGET = each_route\n",
    "\n",
    "    msg = 'Result RMSE is {}, SMAPE is {}, MAE is {}, MSE is {}, R^2 is {}'.format('%.4f' % RMSE(test_df['pred'],df[TARGET]), \n",
    "                                                             '%.4f' % SMAPE(test_df['pred'],df[TARGET]), \n",
    "                                                             '%.4f' % MAE(test_df['pred'],df[TARGET]),\n",
    "                                                             '%.4f' % MSE(test_df['pred'],df[TARGET]),\n",
    "                                                             '%.4f' % metrics.r2_score(df[TARGET],test_df['pred']))\n",
    "    print(msg)\n",
    "    \n",
    "    date_df = df[['Year', 'Month', 'Day', 'Hour']]\n",
    "    date = pd.to_datetime(date_df)\n",
    "    test_df['Date'] = date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
